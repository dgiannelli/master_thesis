%************************************************
\chapter{The Monte Carlo method}\label{ch:mc}
%************************************************

The infrastructure is set up, and the Monte Carlo updating of link variables has to be defined.

In this Chapter will be described how to generate sequences of random variables $x_1, x_2, \ldots$ that are distributed according to a given probability density function $p(x)$.
This methods will then be used in Chapters \ref{ch:local} and \ref{ch:cluster} to sample link variables configurations ${U}$ that follow the weight function $e^{-S^L[U]}$,
and the expectation values of quantum operators of Equation \eqref{eq:lat_exp} can finally be evaluated.

%In this chapter will be firstly introduced the Metropolis-Hastings algorithm,
%and explained how it can be used to compute expectation values.
%
%Then, a specific local algorithm will be defined to compute the integral of Equation \eqref{eq:lat_exp}.
%The first simulation will be run,
%and the result of the plaquette mean value will be compared with the value reported by D\"urr and Hoelbling in 2005 \cite{durr-hoelbling:2005}.
%
%However, since the algorithm is local, at small lattice spacing, the topological freezing will be encountered.

\section{Direct sampling}
\subsection*{Pseudo-random number generators}
A Uniform Pseudo-Random Number Generators (URNG) is a deterministic algorithm that generates erratic and periodic
sequences of all the integers present in an interval $[n_\mathrm{min},n_\mathrm{max}]$, in which $n_\mathrm{min}$ can be $0$ or $1$ and $n_\mathrm{max}$ is a very high number.

A URNG stores in memory a \emph{state}, and the next element of the sequence will be univocally determined by the current state.
A state can be set with an integer number, called \emph{seed}, and a URNG will generate very different sequences for each seed value. 

If the modulus-$n$ operation is taken over the generated numbers, and if $n\ll n_\mathrm{max}$,
the sequence will approximate a uniform random number sequence in the interval $[0,n-1]$.
The quality of a URNG will depend on the the performance of such a sequence over statistical tests.
Anyway, a good URNG should have a very long period $n_\mathrm{max}$ to perform well,
and the length of the period can then be used as a rough measure of its quality.

The URNG that is mostly used in scientific works in the \emph{Mersenne Twister} \cite{mersenne-twister},
because it can generate good quality pseudo-random numbers with a small computational cost.
It has the extremely high period of $2^{19937}-1$ and it is the generator that will be used for this work.

Another common URNG is the \emph{ranlux}, which was introduced specifically for lattice field theories \cite{ranlux}.

The C++ standard library establishes a flexible interface for random numbers generation:
the algorithms can be written in terms of probability distributions,
and the random engine specified later at compile time.
For this work, the same pattern will be preserved.

%\subsection*{Uniform variables}
%Sampling pseudo-random variables from a uniform distribution in $[a,b)$ with a URNG is straightforward,
%since every number of the sequence can be considered to have the same probability of being extracted.
%Calling $n$ a number generated by the URNG, then:
%\paragraph{Integer variables:} 
%%Let $[a,b)$ be an interval of integers and $\mathcal U^\mathrm{int}_{[a,b)}$ be the uniform probability mass function
%Let $[a,b)$ be an interval of integers.
%The modulus operation confines $n$ in the correct interval, indeed:
%%in this interval. The modulus operation confines $n$ in the correct interval, indeed:
%\[
%    n_{[a,b)} \equiv a + \frac{n \mod (b-a)}{b-a}% \quad \mathrm{follows} \quad \mathcal U^\mathrm{int}_{[a,b)}
%\]
%is distributed uniformly in $[a,b)$.
%\paragraph{Real variables:}
%It is important to choose appropriately between open or closed intervals,
%since extreme values could cause problems if fed to functions that are not defined over them.
%Letting $\mathcal U^\mathrm{int}_{\mathcal I}$ be the uniform probability density function defined in the interval $\mathcal I$,
%and considering $a$ and $b$ to be real:
%\[\begin{dcases}
%    x_\mathrm{[a,b]} \equiv a + (b-a)\frac{n}{n_\mathrm{max}-n_\mathrm{min}} \quad \mathrm{follows} \quad \mathcal U^\mathrm{int}_{\mathcal I} \\
%    x_\mathrm{(a,b]} \equiv a + (b-a)\frac{n}{n_\mathrm{max}-n_\mathrm{min}} \quad \mathrm{follows} \quad \mathcal U^\mathrm{int}_{\mathcal I} \\
%    prova
%\end{dcases}\]
%
%

\subsection*{One dimensional real distributions}

Generating real variables that are equally distributed in $(0,1)$ is straightforward.
They can just be obtained normalizing the outputs of an URNG.
%Sometimes it is necessary to avoid the extreme values of the interval,
%since it may happen that they are used by functions that are not defined on them.

Having defined a method to sample uniform real variables,
it can be used also to obtain values that follow other real distributions.

Let $x$ be a uniform random variable in $(0,1)$ and $f(x)$ be an invertible differential function of it.
The probability of having $y\equiv f(x)$ in a subset S of $f((0,1))$ is:
\[
    \int_S\mathrm dy\,p(y) = p(y\in S) = \int_{f^{-1}(S)}\mathrm dx = \int_S\mathrm dy\,\left|\frac{\mathrm dx}{\mathrm dy}\right|
\]
Thus, $f(x)$ is distributed as $p(y) = \left|\frac{\mathrm dx}{\mathrm dy}\right| = \left|\frac{\mathrm df^{-1}}{\mathrm dy}\right|$ in the interval $f((0,1))$.

To sample a one dimensional real distribution $p(y)$ defined in an interval $(y_0, y_1)$,
it is then sufficient to find a monotonically increasing differentiable function $f(x)$ such that $\frac{\mathrm df^{-1}}{\mathrm dy}=p(y)$, $f(0)=y_0$ and $f(1)=y_1$.
Applying such a function to uniform variables $x$ in $(0,1)$ will indeed produce variables $y=f(x)$ distributed according to $p(y)$.

Having defined $F(y) \equiv \int_{y_0}^y\mathrm dy'\,p(y')$, its inverse $F^{-1}(x) \equiv f(x)$ satisfies these conditions:
\[\begin{dcases}
    \left|\frac{\mathrm df^{-1}}{\mathrm dy}\right| = \left|\frac{F(y)}{\mathrm dy}\right| = \frac{F(y)}{\mathrm dy} = p(y) \\
    F(y_0) = 0 \\
    F(y_1) = 1
\end{dcases}\]

With this method, it is possible to sample variables directly from them very efficiently,
but it is limited only to one dimensional distributions that can be integrated and inverted analytically.

To generate variables that follow a generic real distribution, it is necessary to use accept/reject algorithms,
or to sample it with Markov chains.

\section{Markov chain sampling}

\subsection*{Regular Markov chains}
Let $x_1, x_2, x_3, \ldots \in \Omega$ be a sequence of random variables.
This sequence is a \emph{Markov chain} if the probability density function of the $(i+1)^\mathrm{th}$
variable depends only on the value of the $i^\mathrm{th}$ variable:
\[
    p(x_{i+1}|x_1, \ldots, x_i) = p(x_{i+1}|x_i) \quad \forall i\in\mathbb N^*
\]

In the following discussion will be only considered Markov chains that are:
\begin{itemize}
    \item \emph{Stationary}:
        \[
            p(x_{i+1}|x_i) = p(x_{k+i+1}|x_{k+i}) \quad \forall k\in\mathbb N^*
        \]
    \item \emph{Ergodic}:
        \[
            \forall x,y \in \Omega,\ \exists n\in\mathbb N^* : p(x_n=y|x_1=x) \neq 0
        \]
    \item \emph{Aperiodic}:
        \[
            \forall x,y \in \Omega,\ \nexists t\in\mathbb N^* :
            \begin{dcases}
                p(x_n=y|x_1=x)=0 \quad \forall n\neq t,2t,\ldots \\
                p(x_n=y|x_1=x)\neq0 \quad \forall n=t,2t,\ldots
            \end{dcases}
        \]
\end{itemize}

A Markov chain is univocally determided by its \emph{transition probability} from point $x$ to $y$, which is defined as:
\begin{equation}\label{eq:transition}
    w(x \to y) \equiv p(x_{i+1}=y|x_i=x)
\end{equation}

It is proven \cite{mc-mt} that there exists a unique probability density function $p(x\in\Omega)$
such that:
\begin{equation}\label{eq:equilibrium}
	p(x) = \int\mathrm dy\,w(y \to x)p(y)
\end{equation}
and $p(x)$ is called the \emph{equilibrium distibution} (or \emph{stationary distribution}) of the Markov chain.

\subsection*{Convergence of Markov chains}
The usefulness of Markov chains for statistical sampling is guaranteed by two important theorems:
the convergence theorem and the ergodic theorem. The proof of them can be found in \cite{mc-mt}.

The convergence theorem states that the probability density function of a variable that is $n$ steps forward in the Markov chain
converges to the equilibrium distribution $p(x)$, independently from the initial value $x_1$:
\begin{theorem}[Convergence Theorem]\label{th:convergence}
    \[
        \forall x_1 \in \Omega,\ p(x_{n}=x|x_1) \xrightarrow{n\to\infty} p(x)
    \]
    and the convergence rate is exponential:
    \[
        \sup_{x\in\Omega} |p(x_n=x|x_1) - p(x)| \stackrel{n\to\infty}{\scalebox{2}[1.25]{$\sim$}} e^{-\sfrac{n}{\tau_\mathrm{mix}}}
    \]
    where $\tau_\mathrm{mix}$ is the \emph{mixing time}.
\end{theorem}

The convergence theorem establishes a first connection between variables that are directly sampled from $p(x)$
and those which come from a Markov chain at equilibrium in $p(x)$.

This parallelism is completed with the ergodic theorem.
Let $f$ be a function of the random variable $x\in\Omega$ distributed according to $p(x)$.
The arithmetic mean of $f$ evaluated over a Markov chain in equilibrim at $p(x)$ converges to the mean value of $f(x)$:
\begin{theorem}[Ergodic theorem]\label{th:ergodic}
    \[
        \overline f_n \equiv \frac{1}{n}\sum_{i=1}^n f(x_i) \xrightarrow{n\to\infty} \left<f\right>
    \]
\end{theorem}

The ergodic theorem justifies the introduction of all the Markov chain machinery,
since it provides an alternative method to sample variables from a distribution and computing mean values over them.
Directly sampling is, in general, more efficient than Markov chain sampling,
but it is possible to directly sample only a very reduced class of distributions.
With Markov chain sampling, on the other hand, there are no significant restrictions, apart from efficiency.

In practical applications, the arithmetic mean of Theorem \ref{th:ergodic} is not performed over all values of the Markov chain,
and the first values are discarded.
This is justified by the fact that the first value of Markov chain is usually taken arbitrarily,
and it is therefore often very distant from the mean value and the region of typical fluctuations of the distribution.
The sequence takes then few iterations to get closer to the mean value, and, if these values are evaluated,
the precision of the mean value obtained will be heavily reduced.
The number of iterations necessary for the Markov chain to reach typical values of the distribution is called \emph{thermalization time}.
It is, in general, difficult to quantify, and the number of values to discard is usually chosen a posteriori, during the data analysis phase \cite{numerical_recipes}.


\subsection*{Autocorrelation}
However, Markov chain values are correlated,
and, even though this fact does not affect the unbiasedness of the average (Theorem \ref{th:ergodic}),
it has to be taken into account when choosing correct estimators of other quantities.%, such as the variance of the average $\overline f_n$, for example.

The autocorrelation function of $f(x\in\Omega)$ is:
\begin{align*}
    C_f(h) &\equiv \frac{\left<f(x_k)-\left<f\right>\right>\left<f(x_{k+h})-\left<f\right>\right>}{\sigma_f^2} \\
           &= \frac{\left<f(x_k)f(x_{k+h})\right>-\left<f\right>^2}{\sigma_f^2}
\end{align*}
and, using Theorem \ref{th:convergence}, it is possible to evaluate its asymptotycal behaviour.
In fact:
\begin{align*}
    \left<f(x_k)f(x_{k+h})\right> &= \int\mathrm dx_k\int\mathrm dx_{k+h}f(x_k)f(x_{k+h})p(x_k)p(x_{k+h}|x_k) \\
                                  &= \left<f\right>^2 + \mathcal O\left(e^{-\sfrac{h}{\tau_\mathrm{mix}}}\right)
\end{align*}
$C_f$ then decays exponentially with the number of forward Markov steps:
\begin{equation}\label{eq:autocorr_decay}
    C_f(h) \stackrel{h\to\infty}{\scalebox{2}[1.25]{$\sim$}} e^{-\sfrac{h}{\tau_\mathrm{mix}}}
\end{equation}

The first encounter with the autocorrelation function occurs when evaluating the variance of the average $\overline f_n$ of Theorem \ref{th:ergodic}:
\begin{align*}
    \sigma_{\overline f_n}^2 &= \left<\left(\frac{1}{N}\sum_if(x_i) - \left<f\right>\right)^2\right> \\
                             &= \left<\frac{1}{N^2}\left(\sum_if(x_i)\right)^2 +\left<f\right>^2 -\frac{2}{N}\left<f\right>\sum_if(x_i)\right> \\
                             %&= \frac{1}{n^2}\sum_{ij}\left<(f(x_i)-\left<f\right>)(f(x_j)-\left<f\right>)\right> \\
                             &= -\left<f\right>^2 + \frac{1}{N^2}\sum_{ij}\left<f(x_i)f(x_j)\right> \\
                             &= -\left<f\right>^2 + \frac{1}{N}\left<f^2\right> + \frac{1}{N^2}\sum_i\sum_{j \neq i}\left<f(x_i)f(x_j)\right>
\end{align*}
Using the symmetry $i \leftrightarrow j$ and the stationarity condition:
\begin{align*}
    \sum_i\sum_{j\neq i}\left<f(x_i)f(x_j)\right> &= 2\sum_i\sum_{j>i}\left<f(x_i)f(x_j)\right> \\
                                                  &= 2\sum_k\sum_{h=1}^{n-k}\left<f(x_k)f(x_{k+h})\right> \\
                                                  &= 2\sum_k\sum_{h=1}^{n-k}\left(\sigma_f^2C_f(h)+\left<f\right>^2\right) \\
                                                  &= N(N-1)\left<f\right>^2 + 2\sigma_f^2\sum_k\sum_{h=1}^{n-k}C_f(h)
\end{align*}
and using Equation \eqref{eq:autocorr_decay}:
\[
    \sum_k\sum_{h=1}^{N-k}C_f(h) \xrightarrow{N\gg\tau_\mathrm{mix}} \sum_k\sum_{h=1}^\infty C_f(h) = N\tau_f^\mathrm{int}
\]
where $\tau_f^\mathrm{int} \equiv \sum_{h=1}^\infty C_f(h)$ is the \emph{integrated autocorrelation time} and
it is a good measure for the amount of autocorrelation of the Markov chain $f(x_1), f(x_2), \ldots$.
The result for the variance in the limit of a large sample size is then:
\begin{equation}\label{eq:variance}\begin{aligned}
    \sigma_{\overline f_n}^2 &\xrightarrow{n\gg\tau_\mathrm{mix}}-\left<f\right>^2+\frac{1}{N}\left<f^2\right>+\frac{N-1}{N}\left<f\right>^2+\frac{2}{N}\sigma_f^2\tau_\mathrm{int} \\
                             &= \sigma_f^2\frac{1+2\tau_f^\mathrm{int}}{N} = \frac{\sigma_f^2}{N_f^\mathrm{eff}}
\end{aligned}\end{equation}
where $N_f^\mathrm{eff} \equiv N/(1+2\tau_f^\mathrm{int})$ is the \emph{effective number} of uncorrelated values,
since the variance of the average for uncorrelated data would have been $\sigma_f^2/N$.

A good measure for the effectiveness of a Markov chain algorithm can then be the number of effective uncorrelated values produced in a certain amount of computer time,
and it can be used to choose simulation parameters, as it will be done in Chapter \ref{ch:results}.
The most efficient and simple way to evaluate $\sigma_{\overline f_n}^2$, $n_f^\mathrm{eff}$, and $\tau_f^\mathrm{int}$ is with the use of \emph{resampling methods}.
All the results present in this work are obtained with the \emph{binning} or the \emph{jackknife} resampling methods,
and the details of the implementation are showed in Appendix \ref{ap:data}.

\subsection*{Balance equations}
%In the last section, it was showed how Theorems \ref{th:convergence} and \ref{th:ergodic} allow to sample a generic probability density function $p(x)$
%with Markov chains that have $p(x)$ as their equilibrium distribution.
What is still missing in the discussion is how to define a Markov chain whose equilibrium distribution is exactly $p(x)$.

To define a Markov chain, it is sufficient to specify the transition probability of Equation \eqref{eq:transition},
and the Metropolis-Hastings algorithm provides a way to do it.

The equilibrium condititon of Equation \eqref{eq:equilibrium}
can be interpreted as a requirement for stability of the probability flux.
In fact, using the normalization condition $\int\mathrm dy\,w(x \to y) = 1$,
it is equivalent to:
\begin{equation}\label{eq:global_balance}
	\int\mathrm dy\,w(x \to y)p(x) = \int\mathrm dy\,w(y \to x)p(y)
\end{equation}
which is the \emph{global balance} equation.
It states that the probability flux coming from point $x$
is equal to the probability flux going toward point $x$ from all other points.

A sufficient condition for Equation \eqref{eq:global_balance} is:
\begin{equation}\label{eq:detailed_balance}
	p(x)w(x \to y) = p(y)w(y \to x) \quad \forall x,y\in\Omega
\end{equation}
which is the \emph{detailed balance} equation.
Here, the probability flux is in balance for every pair of points.

If any of these two equations for $w(x \to y)$ is satisfied with a given $p(x)$,
then, $p(x)$ will be the equilibrium distribution for the Markov chain.

The easiest to solve of the balance equations is the detailed balance \eqref{eq:detailed_balance},
and, usually, to sample a distribution $p(x)$, it is chosen a regular Markov chain that satisfies it.
Metropolis-Hastings and heat-bath algorithms are two common prescriptions for building such a Markov chain.

\subsection*{Metropolis-Hastings algorithm}
Let $\mathcal P(x\to y)$ be a probability density function called \emph{proposal} probability,
and $\mathcal A(x\to y)$ be a probability called \emph{acceptance} probability.

The idea of Metropolis-Hastings algorithm is the following:
\begin{itemize}
    \item At each step $x$ of the Markov chain, a new value $y$, sampled from $\mathcal P(x\to y)$,
        is proposed to be the next value of the succession, and accepted with a probability $\mathcal A(x\to y)$.
    \item If the proposal is accepted, the Markov chain will indeed move to $y$
    \item If rejected, the next value will be $x$, \ie the same value as the preceding step.
\end{itemize}

With such a prescription, the transition probability is:
\begin{equation}\label{eq:metropolis_transition}
    w(x\to y) = 
    \begin{dcases}
        \mathcal P(x\to y)\mathcal A(x\to y) \quad \mathrm{for}\ y \neq x \\
        \int\mathrm dy'\,\left[1 - \mathcal P(x\to y')\mathcal A(x\to y')\right] \quad \mathrm{for}\ y=x
    \end{dcases}
\end{equation}

The acceptance probability $\mathcal A(x\to y)$ can than be chosen so that the detailed balance condition is satisfied.
For $y=x$, the condition is trivially satisfied. 
For $y\neq x$, the two members of Equation \eqref{eq:detailed_balance} are:
\[
    \begin{dcases}
        p(x)w(x\to y) = \mathcal A(x\to y)p(x)\mathcal P(x\to y) \\
        p(y)w(y\to x) = \mathcal A(y\to x)p(y)\mathcal P(y\to x)
    \end{dcases}
\]
and, a solution for $\mathcal A(x\to y)$ is:
\begin{equation}\label{eq:acceptance}
    \mathcal A(x\to y) = \min\left\{\frac{p(y)\mathcal P(y\to x)}{p(x)\mathcal P(x\to y)},1\right\}
\end{equation}

If the proposal distribution is symmetric, that is, if $\mathcal P(x\to y) = \mathcal P(y\to x)$,
then, the acceptance of Equation \eqref{eq:acceptance} reduces to:
\begin{equation}\label{eq:acceptance_sym}
    \mathcal A^\mathrm{sym}(x\to y) = \min\left\{\frac{p(y)}{p(x)},1\right\}
\end{equation}
which means that the proposal $x\to y$ will be always accepted if $p(y) > p(x)$.
In statistical mechanics, the distribution to sample is usually the Boltzmann distribution $p(x) = e^{-\beta E(x)}$,
which means that the proposal is always accepted when it causes a decrease of the energy.
In lattice gauge theories the same statement still holds, with the action that takes the role of the energy.

What determines the algorithm is then the choice of $\mathcal P(x\to y)$,
and it has to satisfy the following requirements:
\begin{itemize}
    \item The resulting Markov chain must satisfy the regularity conditions.
    \item It must be possible to directly sample $\mathcal P(x\to y)$ to propose values accordingly.
\end{itemize}

With them satisfied, the remaining arbitrariness of $\mathcal P(x\to y)$ leaves space to algorithm optimization.
The sampling of $p(x)$ will be as efficient as how much fast the Markov chain will be to travel across all the space $\Omega$,
and there are two possible ways to boost it:
\begin{itemize}
    \item The proposals should not be too close to the starting position, favoring then broader jumps over narrower ones.
    \item The acceptance $\mathcal A(x\to y)$ should be as high as possible, since, at each proposal rejection,
        the Markov chain will be stuck in the same position.
\end{itemize}
Usually, both of these strategy cannot be pursued all together, then, in this case, a well-balanced compromise should be found.

There is, however, a way to improve the acceptance without reducing the speed of the Markov chain.
In fact, choosing a $\mathcal P(x\to y)$ that is similar to $p(y)$ would let the ratio of Equation \eqref{eq:acceptance} to be closer to one.
Pushing this similarity to the limit case would make the algorithm to actually be a direct sampling, when $\mathcal P(x\to y)$ is equal to $p(y)$.
This strategy will be implemented in Chapters \ref{ch:local} and \ref{ch:cluster} to build an improved version of the local and the cluster algorithm.

\subsection*{Heat-bath algorithm}
If the domain $\Omega$ is multidimensional, and that is always the case in lattice gauge theories, then,
the probability density function $p(x)$ can be very complicated to be sampled efficiently.

However, if $p(x)$ is expressed in terms of a subset of components $x_\mathrm{sub}$, and all the others $x_\mathrm{fix}$ are kept fixed (the heath-bath),
it may be possible that the resulting distribution can be directly sampled, or, at least,
can be sampled efficiently with the Metropolis-Hastings algorithm.

The strategy of the heat-bath algorithms is then to consider only one subset $x_\mathrm{sub}$ at a time, and update them independently.

\begin{description}
    \item{\emph{Ergodicity:}} The marginalization of the subsets causes an increase of the autocorrelation time,
        but the ergodicity is satisfied anyway if the union of all subsets considered constitutes the entire set,
        even though, in some conditions, this marginalization may cause a critical slowing down.
    \item{\emph{Aperiodicity:}} The algorithm that chooses which subset has to be updated can also be periodic,
        since aperiodicity will be guaranteed by the probabilistic nature of the local update.
    \item{\emph{Detailed balance:}} The transition probability for a Markov step $(x_\mathrm{sub},x_\mathrm{fix})\to (y_\mathrm{sub},x_\mathrm{fix})$ is:
        \[
            w(x_\mathrm{sub}\to y_\mathrm{sub}) = p(y_\mathrm{sub};x_\mathrm{fix}) = \int\mathrm dx_\mathrm{fix}\,p(y_\mathrm{sub},x_\mathrm{fix})
        \]
        and the two members of Equation \eqref{eq:detailed_balance} are then equal:
        \[
            \begin{dcases}
                w(x_\mathrm{sub}\to y_\mathrm{sub})p(x_\mathrm{sub},x_\mathrm{fix}) = \int\mathrm dx_\mathrm{fix}\,p(y_\mathrm{sub},x_\mathrm{fix})p(x_\mathrm{sub},x_\mathrm{fix}) \\
                w(y_\mathrm{sub}\to x_\mathrm{sub})p(y_\mathrm{sub},x_\mathrm{fix}) = \int\mathrm dx_\mathrm{fix}\,p(x_\mathrm{sub},x_\mathrm{fix})p(y_\mathrm{sub},x_\mathrm{fix})
            \end{dcases}
        \]
\end{description}

The heat-bath and Metropolis-Hastings algorithms can be combined together,
since the marginal distribution $p(y_\mathrm{sub};x_\mathrm{fix})$ can be sampled with a Metropolis-Hastings step.
This kind of procedure is usually referred to as a \emph{local Metropolis-Hastings},
and the local and cluster algorithms of Chapters \ref{ch:local} and \ref{ch:cluster} will be two instances of it.

The name of heat-bath algorithm is, instead, usually used when the marginal distribution $p(y_\mathrm{sub};x_\mathrm{fix})$ is directly sampled.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
