%*******************************************************
% Preface
%*******************************************************
\phantomsection
\markboth{\spacedlowsmallcaps{Introduction}}{\spacedlowsmallcaps{Introduction}}
\pdfbookmark[1]{Introduction}{Introduction}
\addcontentsline{toc}{chapter}{\tocEntry{Introduction}}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Introduction}

\section*{The renascence of statistical sampling}
Nicholas Metropolis defined in this way \cite{metropolis:1987} the period that began in 1945, after the building of ENIAC, the first general purpose electronic computer.
Its first usage was related to the problem of nuclear chain reaction.
The immediate success brought a renascence of a mathematical technique known as statistical sampling, and its usage spread with a new name: the Monte Carlo method.
This name was used in scientific literature for the first time in 1949 by Metropolis and Ulam \cite{metropolis-ulam:1949}.

The main idea of the Monte Carlo method is based on pseudorandom numbers, which are sequences of integer numbers whose properties approximate the properties of random number sequences.
From pseudorandom numbers, it is possible to sample sequences of real numbers that are distributed according to a given probability density function.
The applications of this principle are very broad: from numerical approximation of integrals and differential equations, to simulation of events that have a probabilistic nature.

The impact on Physics was enormous. Theoretical Physics not only has a powerful method to compute the predictions of a theory, but it is also possible to choose
from a wide-ranging class of theories, through simulation, the one that is most compatible with experiments.
The Monte Carlo method is also an essential tool for Experimental Physics: it is possible to simulate an experimental setup to estimate its performance, or to compare the results with the actual experiments to find out if there are problems with the apparatus, or maybe, to discover weakness in physical theories, and start working to expand them.

\section*{From local algorithms to cluster algorithms}
A class of problems in which the Monte Carlo method shines is the integration in a very high number of dimensions.
Other methods, like quadrature rules, suffer the so called \emph{curse of dimensionality}:
the error of the approximation in terms of the number of function evaluations scales exponentially with the number of dimensions.
For Monte Carlo integration, on the other hand, the scaling is independent from the number of dimensions.

A branch of Physics in which such integrals are very frequent is Statistical Mechanics.
In order to compute the mean value of an observable at thermodynamic equilibrium,
it is necessary to integrate the observable in all configurations of the phase space, weighting them in accordance with the Boltzmann distribution.

A very important step forward in this topic was achieved when Metropolis et al.\@ introduced in 1953 \cite{metropolis:1953} the Metropolis algorithm,
which was further generalized by Hastings in 1970 \cite{hastings:1970} and it is usually referred as Metropolis-Hastings algorithm.
Applying an accept/reject principle, it allows to generate Markov chains of configurations of the phase space that follow a generic probability density function.
It makes then possible to generate configurations in accordance with the Boltzmann distribution,
and the expectation value of an observable can be approximated with its average over these configurations.
This procedure is in general far simpler and more efficient than generating all possible configurations and weighting them afterwards.

An important challenge in defining Markov chains is making them \emph{ergodic},
\ie giving them the possibility to eventually reach every point in the phase space in a finite number of steps.
Without ergodicity, the Markov chain is actually sampling only a subset of the domain, thus, the integral result will probably be incorrect.
Even if the algorithm is ergodic in principle,
it can effectively behave as it was not ergodic if the number of steps needed to reach all relevant sectors of the phase space is too high
with respect to the number of iterations the computer can perform in the available amount of time.

An example of this problem is the two dimensional Ising model near the \emph{critical temperature}.
This system comprises a lattice of spins that can be oriented in two possible directions.
Each pair of adjacent spins gives a contribute to the total energy: if they are aligned, the contribute is negative, otherwise it is positive.
At thermodynamic equilibrium, if the temperature is above the critical temperature and in absence of external magnetic fields, the mean magnetization of the system is zero.
Below the critical temperature, most of the spins are aligned in the same direction, producing a non-zero mean magnetization.
Varying the temperature in a range of values that are still greater than the critical temperature, but very close to it,
it is possible to observe a critical behaviour: distinct clusters of spins aligned internally all together start arising.
The size of these clusters keeps increasing as the temperature approaches the critical value.

In terms of a Monte Carlo simulation, this cluster structure has important implications.
The energy of the system has a local nature,
\ie it is expressed in terms of the single spins, and, modifying the value of a spin,
the variation of the total energy is only due to the local energy variation.
Because of this fact, the most straightforward way to sample all the configurations is to update all the spins one by one,
changing only one spin at each iteration of the program. This is what is called a local Monte Carlo.
However, when the system is divided into clusters, local algorithms show their weakness.
Updating the spins one per time makes very unlikely that a whole cluster can be reversed,
and this problem gets worse when the clusters size increases.
This is the \emph{critical slowing down}: the correlation length between configurations increases exponentialy approaching toward the critical temperature.

A solution to the critical slowing down for the Ising model\footnote{It applies also to the more general Potts model} was found by Swendsen and Wang in 1987 \cite{swendsen-wang:1987}.
They found a way to sample the Boltzman distribution inverting many contiguous and parallel spins all together.
It is recorded as the first \emph{cluster algorithm}.
In 1989, Wolff \cite{wolff:1989} improved this algorithm and extended it also to continuous spin models.

Cluster algorithms have a chance to update big groups of elements, and, if they are well defined,
the size of these groups is scalable together with the typical size of the clusters present in the system.
Critical slowing down can then be heavily dampened, making simulations affordable even in critical conditions.

However, the difficulty in creating cluster algorithms is to find a way to efficiently update big parts of the system without compromising the Boltzmann distribution.
This has to be done exploiting properties of the considered system, and, consequently, the algorithm will be specifically tailored for systems that share these properties.
For example, Wolff algorithm can be extended to the most generic continuous spin system on a lattice,
but any good solution was not found to extend it to the hard sphere model,
\ie a system in which solid spheres can move in space without overlapping.
The first efficient cluster algorithm for this model was found 1995 by Dress and Krauth \cite{dress-krauth:1995}.
It is called \emph{the pivot algorithm} and was formulated specifically for hard spheres and related systems\footnote{Such as binary mixtures or dimers, for example},
independently from the efforts that have been made to extend Wolff algorithm.

Another important case of critical slowing down is \emph{topological freezing}.
It affects Quantum Cromodynamics\footnote{As well as other quantum theories with a non-trivial topological structure}
when it is studied with the Monte Carlo method in its lattice formulation, \ie Lattice QCD.

\section*{Lattice QCD and topological freezing}
\emph{Quantum Cromodynamics} is the quantum field theory that describes the \emph{strong interation},
which concerns the coupling between \emph{quarks} (the constituents of nucleons and mesons) and \emph{gluons} (massless particles, mediators of the interaction).
It is formulated as a non-abelian gauge theory, with gauge group $SU(3)$,
in which quark fields lie in the foundamental representation of the group and gluon fields are the gauge fields.
The conserved charge associated with the gauge group is called \emph{color charge} and it is the analogous of the electric charge in electrodynamics.

This formulation for the theory of strong interation was introduced in 1973 by Fritzsch, Leutwyler and Gell-Mann \cite{fritzsch:1973},
and it is based on the general field theory developed by Yang and Mills in 1954 \cite{yang-mills:1954}.
It came after years of important theoretical contributions that followed the experimental discovery of a growing number of hadrons in the 1950s.

In 1973, Gross, Wilczek \cite{gross-wilczek:1973} and Politzer \cite{politzer:1973} discovered an important property of QCD: \emph{asymptotic freedom}.
As the energy scale increases, the strenght of the interaction decreases,
and physical predictions of the theory can be analytically computed with a perturbative power expansion of the coupling constant.

On the other side of the spectrum, when the energy scale decreases, \emph{color confinement} arises.
Quarks and gluons cannot be isolated and observed directly in normal conditions\footnote{It is only possible at extremely high temperature and/or density, when they form a quark-gluon plasma, a new state of matter},
and they are confined in mesons or nucleons.
As coupling is higher at this scale of energy, perturbation theory is not viable, and non-perturbative methods are needed to compute theory predictions.
Lattice QCD is a different approach that fulfils these requirements, and it was formulated in 1974 by Wilson \cite{wilson:1974}.
Gauge field theories can be quantized on a discrete lattice in euclidean space-time and,
in this framework, it is possible to compute theory predictions in the strong coupling regime.

In 1979, Creutz et al.\@ \cite{creutz:1979} and Wilson \cite{wilson:1980}
had the intuition that gauge theories on a finite euclidean lattice are equivalent to a system at thermodynamic equilibrium,
and mean values of observables can be computed with the same Monte Carlo techniques that had already been employed in Statistical Mechanics.
The continuum limit can then be extrapolated with the \emph{renormalization group} method, performing simulations on finer and finer lattices.
Following these precursor works, lattice QCD simulations produced in short time important results in evaluating quark-antiquark potential and hadron masses
\cite{creutz:1980, hamber-parisi:1981, weingarten:1982}.

However, there is a problem in extrapolating the continuum limit.
QCD has a non-trivial topological structure, and gluon field configurations are partitioned in homotopy classes, characterized by their integer topological charge.
It is then impossible to continuously deform a configuration with a charge value into another with a different charge.
When the theory is discretized on a lattice, it becomes possible to tunnel from one class to another,
though the effort is harder as continuum is approached.

In the context of Markov chain Monte Carlo simulations, homotopy class partitioning results in a loss of ergodicity.
The correlation time of topological charge increases exponentially as the lattice gets finer.
This is an instance of critical slowing down, and it is called \emph{topological freezing}.

The demand for an efficient topological charge sampling method is very high, since it is a problem that is currently not resolved.
It is of a great physical interest, especially because there are particle masses directly related to topological charge:
the $\eta'$ meson mass, whose high value has an explaination within the Witten-Veneziano mechanism \cite{witten:1979-1, witten:1979-2, veneziano:1979},
and the mass of the \emph{axion}, which is an hypothetical particle postulated by Weinberg \cite{weinberg:1978} and Wilczek \cite{wilczek:1978}
in the context of Peccei-Quinn model \cite{peccei-quinn:1977-1, peccei-quinn:1977-2}.
The weakness of the coupling between the axion and the Standard Model makes it a good candidate to be \emph{dark matter}.
The experimental dicovery of the axion could be a great step in understanding the nature of dark matter, which is one of the main goal of modern Physics.
An accurate value of the axion mass from lattice QCD simulations would indicate the point of the energy spectrum in which the axion should be,
and its existence could then be verified experimentally.

The topological charge is the integral of a local quantity, the topological charge density%
\footnote{In QCD, topological charge density rose from studies about anomalous symmetries, as it is proportional to the anomaly of the axial $U(1)$ symmetry},
which is a functional of gauge fields configurations.
Studying its spatial distribution in lattice QCD simulations close enough to the continuum limit, in condition of topological freezing, it exhibits an interesting structure.
Some regions of the space contribute with a certain topological charge, which can be positive or negative, and,
fast forwarding the configurations of the Markov chain,
those regions may deform and move around in the space, but their net contribute to the total charge does not change.
They are the discretized version of instantons, which are exact solutions of the classical equation of motion.
They have a definite integer topological charge, and their existence proves the non-triviality of QCD topology.

Many strategies have been developed to dampen the problem of topological freezing.
For example, it is possible to evaluate the finite volume bias that is introduced in condition of fixed topology,
and reduce it with extrapolations \cite{brower:2003, aoki:2007}.

Otherwise, implementing the lattice with boundary conditions that are open instead of periodic \cite{luescher:2011},
the topological freezing is removed because instantons can be created and destroyed on the border.
However, open boundaries produce an increased finite volume bias, since translational symmetry is lost.

Other strategies that have been proposed are:
the \emph{simulated tempering} \cite{delia:1996},
the \emph{slab} method \cite{slab-1, slab-2},
the usage of anti-periodic boundaries \cite{mages:2017}
and the \emph{multicanonical} method \cite{todaro:2018}.

All these methods have their flaws and virtues, but none of them represent a complete solution to the problem.
The cluster stucture of topologically frozen configurations suggests that a cluster algorithm would be the perfect solution.
However, it hasn't been found yet.

\section*{Toy models for QCD}
Lattice QCD simulations are, in general, difficult to implement and computationally very expensive.
For this reason, many new ideas and algorithms are firstly applied to simplified models that share the same properties.
This can be useful both for testing and benchmarking purposes.

In particular, toy models are very useful if they have exact solutions to the problem, either it is analytical or numerical.
This is of crucial importance for bias evaluation and reduction.
In fact, the same strategies that have been proposed to dampen topological freezing in QCD can be implemented in toy models, and,
if exact solution are available, it is possible to quantify the bias and verify if extrapolations actually converge to correct values.

A toy model suitable to study topological freezing should be a quantum system in which dynamical variables configurations belong to distinct topological sectors,
and, approaching the continuum limit of path integrals, the different sectors should become disconnected with respect to local Monte Carlo algorithms.
The simplest model that satisfies these requirements is a quantum mechanical particle that can move on a circumference.
The topological charge here is equal to the winding number, \ie the number of times a path configuration wraps around the circle.
In this simple model, it is possible to find exact numerical solutions and compare them with the techniques used in lattice QCD \cite{dromard-wagner:2014, bonati-delia:2018}.

There is, however, an important property of QCD topology that the quantum mechanical particle on a circumference does not reproduce.
It is the presence of different instantons that can move and interact in the space.
The simplest toy models that have this properties are two dimensional gauge theories,
and the $U(1)$ pure gauge theory is, among them, the one that has the most straightforward implementation.
It is also possible to include fermions into it, and such a theory is known as the \emph{Schwinger model}.
It is a widely used model since it exhibits fermion confinement, with a potential similar to that of quarks in QCD.

In Chapter \ref{ch:cluster}, a new cluster algorithm for the two dimensional $U(1)$ pure gauge theory will be presented.
It constructs closed paths of lattice links and reverses the sign of all gauge fields configurations inside the region surrounded by the path,
accepting or rejecting the operation with a well-tailored Metropolis-Hastings algorithm.
A chain of local gauge transformations along the path can make the energy variation of the entire move to be the same as with a local update,
making the acceptance of the algorithm be reasonably high and idependent from the cluster size.

This algorithm is not extensible to more than two dimensions,
because regions of space could not be completely surrounded with a one dimensional path,
and it is not possible to create a chain of local gauge transformations that covers all links of a multidimensional surface.
However, the algorithm solves the critical behaviour in two dimensions,
and it is efficient, parallelizable, and simple to implement, especially with modern programming languages techniques.
It yields numerical exact results that can be used to test all the workarounds that are used to fight topological freezing in QCD. 

The only other cluster algorithm for this theory that is present in literature was published in 1992 by Robert Sinclair \cite{sinclair:1992}.
His algorithm uses the local gauge invariance to map the system into a one dimensional XY model, and updates it using the extension of the Wolff algorithm.
Although it is a valid solution, its implementation is more complicated, and it does not leave the choice of cluster size. 
The algorithm of Chapter \ref{ch:cluster} is formulated specifically for this system, making it more adaptable to its properties and easier to implement and debug.
In some ways, the approach is similar to that of Dress and Krauth,
who chose to create a well suited algorithm for a specific task and closer to the underlying Physics, rather than extending Wolff algorithm.\bigskip\bigskip

\noindent
The exposure of this thesis work will fit in the following scheme:
\begin{description}
    \item{Chapter \ref{ch:toy_model}:} Definition of the two dimensional pure gauge $U(1)$ theory and its topological charge,
    both in the continuum space and on a discrete lattice.
    \item{Chapter \ref{ch:lattice}:} Implementation of the lattice theory in C++17.
    \item{Chapter \ref{ch:local}:} Introduction to Markov chain Monte carlo and implementation of a local Metropolis-Hastings algorithm.
    \item{Chapter \ref{ch:cluster}:} Discussion of topological freezing, and introduction of a new cluster Metropolis-Hastings algorithm that solves the problem.
    \item{Chapter \ref{ch:results}:} Numerical results obtained with the cluster algorithm and conclusions.\\
    \item{Appendix \ref{ap:code}:} Complete implementation of the algorithms.
    \item{Appendix \ref{ap:data}:} Methods used for data analysis.
\end{description}

\endgroup

\vfill
