%************************************************
\chapter{Local algorithm}\label{ch:local}
%************************************************

In this Chapter will be defined a local Metropolis-Hastings algorithm that samples the link variables.
Then, a specific local algorithm will be defined to compute the integral of Equation \eqref{eq:lat_exp}.
The first simulation will be run,
and the result of the plaquette mean value will be compared with the value reported by D\"urr and Hoelbling in 2005 \cite{durr-hoelbling:2005}.

However, since the algorithm is local, at small lattice spacing, the topological freezing will be encountered.

\section{Link variables updating}
\subsection*{Heat-bath}
%\begin{align*}
%    S &= \beta\sum_s\left(1-\Re\,U[\mathcal P(s)]\right) \\
%    \Delta S &= -\beta\Re\,\left(U[\mathcal S_0]U[\mathcal L] + U[\mathcal S_1]U[\mathcal L]\right) \\
%             &= -\beta\Re\,(Wu) \\
%             &= -\beta|W|\cos(\phi + \arg W) \\
%             &= e^{-k\cos(x)}
%\end{align*}
The probability density function of links configurations $\{U\}$ is:
\[
    p(U) \propto e^{-S}
\]
in which the action $S$ is:
\[
    S = \beta\sum_s\left(1-\Re\,U[\mathcal P(s)]\right)
\]
Every plaquette value $\Re\,U[\mathcal P(s)]$ depends on four links variables,
and every link variable contributes to two different plaquettes.

Links variables are then simultaneously coupled in their probability density function,
which is then impossible to sample directly.

However, the action has a local nature,
since every link variable contributes only to the two plaquettes that include it.
Thus, using the heat-bath approach and considering only the marginal distribution of one link variable,
the contribution of all plaquettes that do not include the link can be integrated out of the distribution.

Let $\mathcal L$ be the link considered, and $\mathcal P_1, \mathcal P_2$ the two plaquettes that include it.
The two paths $\mathcal S_1, \mathcal S_2$ sketched in Figure \ref{fig:staples}
are called \emph{staples}.
They are all the staples connected to $\mathcal L$,
in the sense that they are the staples that form a plaquette if $\mathcal L$ is included to them.

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[x=5em,y=5em]
        \draw[step=1, help lines, dashed, color=black!30] (0,0) grid (3.5,2.5);
        \draw[->,thick,black] (0,0) -- (3.5,0) node [right] {$x_1$};
        \draw[->,thick,black] (0,0) -- (0,2.5) node [above] {$x_2$};

        \draw[->-,very thick,orange] (2,1) node [black,below] {$\mathcal L$} --  (2,2);

        %left staple
        \draw[->-,very thick,RoyalBlue] (2,2) -- (1,2);
        \draw[->-,very thick,RoyalBlue] (1,2) -- node [black,left] {$\mathcal S_1$} (1,1);
        \draw[->-,very thick,RoyalBlue] (1,1) -- (2,1);

        %right staple
        \draw[->-,very thick,RoyalBlue] (2,2) -- (3,2);
        \draw[->-,very thick,RoyalBlue] (3,2) -- node [black,right] {$\mathcal S_2$} (3,1);
        \draw[->-,very thick,RoyalBlue] (3,1) -- (2,1);

        %left plaq
        \draw[->-,thick,dashed,black] (1.2,1.2) -- (1.8,1.2);
        \draw[->-,thick,dashed,black] (1.8,1.2) -- (1.8,1.8);
        \draw[->-,thick,dashed,black] (1.8,1.8) -- (1.2,1.8);
        \draw[->-,thick,dashed,black] (1.2,1.8) -- (1.2,1.2);
        \node at (1.5,1.5) {$\mathcal P_1$};

        %right plaq
        \draw[->-,thick,dashed,black] (2.2,1.2) -- (2.8,1.2);
        \draw[->-,thick,dashed,black] (2.8,1.2) -- (2.8,1.8);
        \draw[->-,thick,dashed,black] (2.8,1.8) -- (2.2,1.8);
        \draw[->-,thick,dashed,black] (2.2,1.8) -- (2.2,1.2);
        \node at (2.5,1.5) {$\mathcal P_2$};

        %\node[anchor={north east}] at (1,1) {$\mathcal P_1$};
        %\node[anchor={north east}] at (2,1) {$\mathcal P_2$};

    \end{tikzpicture}
    \caption{Staples connected to a vertical link}
    \label{fig:staples}
\end{figure}

The contribution of $\mathcal P_1, \mathcal P_2$ to the action can be written in terms of
the link variable $u \equiv U[\mathcal L]$ in the following way:
\[\begin{aligned}
    \Re\,U[\mathcal P_1] + \Re\,U[\mathcal P_2] &= \Re\,(uU[\mathcal S_1]) + \Re\,(u^*U^*[\mathcal S_2]) \\
                                                &= \Re\,(Wu)
\end{aligned}\]
where $W \equiv U[\mathcal P_1] + U[\mathcal P_2] \notin U(1)$ has a modulus $|W|\neq1$.
Thus, the marginal distribution of $u$ with all the other link variables kept fixed at $u_\mathrm{fix}$ is:
\[\begin{aligned}
    p(u;u_\mathrm{fix}) &\propto e^{-\beta(1-\Re(Wu))} \\
                        &\propto e^{\beta\Re(Wu)} \\
                        &= e^{\beta|W|\Re\left(u_0\right)}
\end{aligned}\]
Here, $u_0 \equiv \frac{W}{|W|}u \in U(1)$, and $\mathrm du_0=\mathrm du$ are then the same Haar measure of the compact group $U(1)$.
Which means:
\[
    p(u;u_\mathrm{fix}) = p(u_0;u_\mathrm{fix}) \propto e^{\beta|W|\Re\left(u_0\right)}
\]
Indeed, $u$ can be parameterized as $u\equiv e^{i\phi}$ in which $\phi$ is defined in $(-\pi + \phi_0, \pi + \phi_0]$ with a generic $\phi_0$.
After a transformation $u \rightarrow \frac{W}{|W|}u$, $\phi$ undergoes the translation $\phi \rightarrow \phi + \arg W$.
If the parameterization is redefined as $\phi_0 \rightarrow \phi_0 - \arg W$, the shift of the definition interval of $\phi$ is then reabsorbed.

Choosing to parameterize $u_0 \equiv e^{ix}$ with $x \in (-\pi, \pi]$,
the marginal distribution of $x$ finally is:
\begin{equation}\label{eq:local_pdf}
    p(x;u_\mathrm{fix}) \propto e^{k\cos(x)} \quad x \in (-\pi, \pi]
\end{equation}
with $k \equiv \beta|W|$.
Equation \eqref{eq:local_pdf} cannot be integrated analytically,
therefore it is not possible to directly sample it.

It was chosen to sample it using a Metropolis-Hastings algorithm,
and, what has been said at the end of Chapter \ref{ch:mc},
the acceptance of the algorithm increases as much similar $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ gets to $p(x_\mathrm{new};u_\mathrm{fix})$.

On this matter, Laplaces's method identifies such a distribution.
Indeed it is its corollary that any unimodal distribution of the form $\propto e^{kf(x)}$ approaches to a Gaussian as $k$ increases.

In particular, it proves that (Figure \ref{fig:gauss}):
\begin{equation}\label{eq:gauss_approx}
    e^{k\cos(x)} \stackrel{k\to\infty}{\scalebox{2}[1.25]{$\sim$}} e^{-kx^2/2}
\end{equation}

\begin{figure}[!htb]
    \centering
    \input{gfx/gauss.pgf}
    \caption{$p(x;u_\mathrm{fix})$ converges to a Gaussian as $k$ increases}
    \label{fig:gauss}
\end{figure}

Laplace's method states that, if $f(x)$ is a twice-differentiable function that has only one maximum point $x_0 \in [a,b]$ with $x_0 \neq a,b$,
then:
\begin{equation}\label{eq:laplace}
	\int_a^b\mathrm dx\,e^{kf(x)} \xrightarrow{k\to\infty} e^{kf(x_0)}\int_a^b\mathrm dx\,e^{kf"(x_0)(x-x_0)^2/2}
\end{equation}
Indeed, as $k$ increases, the points that are further from the maximum point $x_0$ are exponentially suppressed,
and only the points in the range of $x_0$ becomes relevant to the integral.
The order zero of the Taylor expansion of $f(x)$ around $x_0$ is a multiplicative constant and the order one does not contribute since it is symmetric.
The meaningful part is then the second order, which is negative since $x_0$ is a maximum,
and hence the integrand of the second term of Equation \eqref{eq:laplace} is a Gaussian.

Applying Laplace's method for the case of Equatiton \eqref{eq:local_pdf},
the global maximum is $x_0=0$, and $f"(x_0) = -1$.
Thus:
\[
    \int_{-\pi}^x\mathrm dx'\,e^{k\cos(x')} \stackrel{k\to\infty}{\scalebox{2}[1.25]{$\sim$}} \int_{-\pi}^x\mathrm dx'\,e^{-k{x'}^2/2} \quad \mathrm{for}\ x>0
\]
and equation \eqref{eq:gauss_approx} is recovered taking the derivative of both members, and extending the results for symmetry around $0$.

\subsection*{Gaussian sampling}
The proposal distribution has then been chosen to be:
\begin{equation}\label{eq:gauss_proposal}
    \mathcal P(y\to x) \propto e^{-kx^2/2} \quad x \in (-\pi,\pi]
\end{equation}

A Gaussian in $(-\infty,\infty)$ can be directly sampled using the Box-Muller method,
and an adaptation of it is needed to restrict the variable into $(-\pi,\pi)$.

The probability density function of two independent random variables $x_1,x_2$ distributed according to $\mathcal P(y\to x)$ is:
\begin{equation}\label{eq:gauss2d}
    p(x_1,x_2) \propto e^{-\frac{k}{2}(x_1^2+x_2^2)}
\end{equation}

Switching to polar coordinates\footnote{The correct extreme values of the interval of definition will be recovered afterwards, adapting the final solution.}:
\[\begin{aligned}
    &\begin{dcases}
        x_1 = r \cos\theta \\
        x_2 = r \sin\theta
    \end{dcases}
    &
    \begin{dcases}
        r \in (0,\pi] \\
        \theta \in [0,2\pi)
    \end{dcases}&
\end{aligned}\]
the distribution in terms of $r$ and $\theta$ becomes:
\[
    p(r,\theta) \propto re^{-\frac{k}{2}r^2}
\]
which is separable into two independent distributions:
\[\begin{aligned}
    p(r,\theta) &= p(r)p(\theta) \\
                &= \left(\mathcal Nre^{-\frac{k}{2}r^2}\right)\left(\frac{1}{2\pi}\right)
\end{aligned}\]
where $\mathcal N$ is a normalization constant.

$p(\theta)$ is a uniform distribution and is then trivial to sample.
To directly sample $p(r)$, instead, can be used the procedure defined in Chapter \ref{ch:mc}.
Indeed, integrating it:
\[
    F(r) = \int_0^r\mathrm dr'\,p(r') = \frac{\mathcal N}{k}\left.e^{-\frac{k}{2}{r'}^2}\right|_r^0 = \frac{\mathcal N}{k}\left(1-e^{-\frac{k}{2}r^2}\right)
\]
and evaluating its inverse $F^{-1}(y_1)$:
\[\begin{aligned}
    y_1 &= F(r) = \frac{\mathcal N}{k}\left(1-e^{-\frac{k}{2}r^2}\right) \\
        &\Rightarrow e^{-\frac{k}{2}r^2} = 1 - y_1\frac{k}{\mathcal N} \\
        &\Rightarrow F^{-1}(y_1) = r = \sqrt{-\frac{2}{k}\log\left(1-\frac{k}{\mathcal N}y_1\right)}
\end{aligned}\]

It remains to evaluate the normalization constant:
\[
    \mathcal N^{-1} = \int_0^\pi\mathrm dr\,re^{-\frac{k}{2}r^2} = \frac{1}{k}\left(1-e^{-\frac{k}{2}\pi^2}\right)
\]
to get:
\[
    F^{-1}(y_1) = \sqrt{-\frac{2}{k}\log\left[1-y_1\left(1-e^{-\frac{k}{2}\pi^2}\right)\right]}
\]
which means that if $y_1, y_2$ are equally distributed in $(0,1)$, then:
\begin{equation*}
    \begin{dcases}
        x_1 = F^{-1}(y_1)\cos(2\pi y_2) \\
        x_2 = F^{-1}(y_1)\sin(2\pi y_2)
    \end{dcases}
\end{equation*}
are two independent variables distributed according to \eqref{eq:gauss2d}.

However they do not span homogeneously all the interval $(-\pi,\pi]$ if the probability of being exactly at points $0$ and $\pi$ is taken into account,
which is non-zero in numerical applications because the representation of numbers in computer memory is finite.

To obtain a random variable $x = r\cos \theta \in (-\pi,\pi]$,
it is necessary that $r \in [0,\pi]$ and $\theta \in (-\pi,\pi)$.

Since $F^{-1}(0)=0$ and $F^{-1}(1)=\pi$,
a completely uniform variable $x$ distributed according to Equation \eqref{eq:gauss_proposal} is given by:
\begin{equation}\label{eq:gauss_angle}\begin{gathered}
    x = \sqrt{-\frac{2}{k}\log\left[1-y_1\left(1-e^{-\frac{k}{2}\pi^2}\right)\right]}\cos\left[2\pi\left(y_2-\frac{1}{2}\right)\right] \\
    \mathrm{with}\ \begin{dcases}
                         y_1 \in [0,1] \\
                         y_2 \in (0,1)
                   \end{dcases}
\end{gathered}\end{equation}

\subsection*{Local Metropolis-Hastings}
Now that the proposal distribution $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ has been chosen,
the local algorithm can be defined as follows:
\begin{itemize}
    \item Select one link $\mathcal L$.
    \item Identify $\mathcal S_1$ and $\mathcal S_2$, the staples connected to $\mathcal L$ (Figure \ref{fig:staples}).
    \item Compute $W$, and then $k$ and $x_\mathrm{old}$:
        \[\begin{aligned}
            &W = U[\mathcal S_1] + U[\mathcal S_2],
            &
            &\begin{dcases}
                k = \beta |W| \\
                x_\mathrm{old} = \arg\left(WU[\mathcal L]\right)
            \end{dcases}
        \end{aligned}\]
    \item Extract $x_\mathrm{new}$ from $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ using Equation \eqref{eq:gauss_angle}.
    \item Accept the proposal with probability (Equations (\ref{eq:acceptance}, \ref{eq:local_pdf}, \ref{eq:gauss_proposal})):
        \[\begin{aligned}
            \mathcal A(x_\mathrm{old}\to x_\mathrm{new})
                 &=  \min\left\{\frac{p(x_\mathrm{new};u_\mathrm{fix})\mathcal P(x_\mathrm{new}\to x_\mathrm{old})}
                                     {p(x_\mathrm{old};u_\mathrm{fix})\mathcal P(x_\mathrm{old}\to x_\mathrm{new})},1\right\} \\[.5em]
                 &= \min\left\{e^{k\left(\cos(x_\mathrm{new})-\frac{1}{2}x_\mathrm{old}^2-\cos(x_\mathrm{old})+\frac{1}{2}x_\mathrm{new}^2\right)},1\right\}
        \end{aligned}\]
        That is, generate a random variable $p$ from the uniform distribution in $[0,1)$,
        and accept the proposal if:
        \[
            p < e^{k\left(\cos(x_\mathrm{new})-\frac{1}{2}x_\mathrm{old}^2-\cos(x_\mathrm{old})+\frac{1}{2}x_\mathrm{new}^2\right)}
        \]
    \item If the proposal is accepted, set the corresponding new value for the link variable $U[\mathcal L]$
        \[
            U[\mathcal L] \mapsfrom e^{i(x_\mathrm{new}-\arg W)}
        \]
\end{itemize}

The local algorithm should then iterate over all links in the lattice to be ergodic, as discussed in Chapter \ref{ch:mc}.
Every sweep over all lattice links will be considered a step of the Markov chain,
and the expectation values will be evaluated on this sequence.

\subsection*{Local algorithm implementation}
Here will be provided a C++17 implementation of the local algorithm using the {\ttfamily Lattice class} introduced in Chapter \ref{ch:lattice}.

However, it is necessary to define the interface to sample the proposal distribution of Equation \eqref{eq:gauss_proposal} and the uniform distribution.

In the C++ Standard Library, probability distributions are defined in terms of a generic templated type {\ttfamily URNG},
and the same convention will be used here.

The uniform distribution is alredy present in the Standard implemented as the {\ttfamily uniform\_real\_distribution} class.
So it is just needed to instatiate it with the defaulf interval parameters, which correspond to the interval $[0,1)$:
\begin{lstlisting}[caption={Uniform distribution sampling function}]
// Usage: uniform(URNG) -> x unif. in [0,1)
uniform_real_distribution<> uniform;
\end{lstlisting}

The algorithm of Equation \eqref{eq:gauss_angle} can be implemented using the interface of Standard's URNGs:
\begin{lstlisting}[caption={Gaussian angle distribution sampling function}]
template <class URNG>
double gauss_angle(double k, URNG &rng)
{
	// y1 unif in [0,1], y2 unif in (0,1)
    double y1 = (double)(rng()-rng.min())/(rng.max()-rng.min());
    double y2 = (double)(rng()-rng.min()+1lu)
                         / (rng.max()-rng.min()+2lu);

    double r = sqrt(-2./k*log(1.-y1*(1.-exp(-0.5*k*pi*pi))));
    double theta = 2.*pi*(y2-0.5);

    return r*cos(theta);
}
\end{lstlisting}


\begin{lstlisting}[caption={Staple type}]
struct Staple : array<Link,3>
{
    Staple(Link link, int nextdir)
    {
        array<int,3> dirs = {nextdir,-link.dir,-nextdir};

        auto [x1, x2] = link.site;
        for (int i : {0,1,2}) {
            // Shift x1,x2 along link dirs
            switch (dirs[i]): {
                case 1: x1++; break;
                case -1: x1--; break;
                case 2: x2++; break;
                case -2: x2--; break;
            }
            (*this)[i] = Link{x1,x2,dirs[i]};
        }
    }
};
\end{lstlisting}

\begin{lstlisting}[caption={Connected staples function}]
pair<Staple> connected_staples(Link link)
{
}
\end{lstlisting}

\begin{lstlisting}[caption={Local update function}]
template <class URNG>
void local_update(Lattice lat, Link link, URNG &rng)
{

}
\end{lstlisting}


\begin{equation}
    prova
\end{equation}

\section{First results}

\[
    \input{tables/prova.tex}
\]
\the\textwidth

\subsection{Action}

\subsection{Topological freezing}

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
