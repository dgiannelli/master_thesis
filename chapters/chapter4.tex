%************************************************
\chapter{Local algorithm}\label{ch:local}
%************************************************

In this Chapter a local Metropolis-Hastings algorithm that samples link variables will be defined.
%Then, a specific local algorithm will be defined to compute the integral of Equation \eqref{eq:lat_exp}.
The first simulations have been performed using it,
and the result of the plaquette mean energy will be compared with the value reported by D\"urr and Hoelbling in 2005 \cite{durr-hoelbling:2005}.

Even though the acceptance of the local algorithm is close to its possible maximum value of one,
since the algorithm is local, at small lattice spacing, the topological freezing is encountered.

This causes both the divergence of the charge autocorrelation time,
and the biasedness of observables mean values.

\section{Link variables updating}
\subsection*{Heat-bath}
%\begin{align*}
%    S &= \beta\sum_s\left(1-\Re\,U[\mathcal P(s)]\right) \\
%    \Delta S &= -\beta\Re\,\left(U[\mathcal S_0]U[\mathcal L] + U[\mathcal S_1]U[\mathcal L]\right) \\
%             &= -\beta\Re\,(Wu) \\
%             &= -\beta|W|\cos(\phi + \arg W) \\
%             &= e^{-k\cos(x)}
%\end{align*}
The probability density function of links configurations $\{U\}$ is:
\[
    p(U) \propto e^{-S}
\]
in which the action $S$ is:
\[
    S = \beta\sum_s\left(1-\Re\,U[\mathcal P(s)]\right)
\]
Every plaquette value $\Re\,U[\mathcal P(s)]$ depends on four links variables,
and every link variable contributes to two different plaquettes.

Links variables are then simultaneously coupled in their probability density function,
which is then complicated and impossible to sample directly.

However, the action has a local nature,
since every link variable contributes only to the two plaquettes that include it.
Thus, using the heat-bath approach, and considering only the marginal distribution of one link variable,
the contribution of all plaquettes that do not include the link can be integrated out of the distribution.

Let $\mathcal L$ be the link considered, and $\mathcal P_1, \mathcal P_2$ the two plaquettes that include it.
The two paths $\mathcal S_1, \mathcal S_2$ sketched in Figure \ref{fig:staples}
are called \emph{staples}.
They are all the staples connected to $\mathcal L$,
in the sense that they are the staples that form a plaquette if $\mathcal L$ is included to them.

\begin{figure}[!htb]
    \centering
    \begin{tikzpicture}[x=5em,y=5em]
        \draw[step=1, help lines, dashed, color=black!30] (0,0) grid (3.5,2.5);
        \draw[->,thick,black] (0,0) -- (3.5,0) node [right] {$x_1$};
        \draw[->,thick,black] (0,0) -- (0,2.5) node [above] {$x_2$};

        \draw[->-,very thick,orange] (2,1) node [black,below] {$\mathcal L$} --  (2,2);

        %left staple
        \draw[->-,very thick,RoyalBlue] (2,2) -- (1,2);
        \draw[->-,very thick,RoyalBlue] (1,2) -- node [black,left] {$\mathcal S_1$} (1,1);
        \draw[->-,very thick,RoyalBlue] (1,1) -- (2,1);

        %right staple
        \draw[->-,very thick,RoyalBlue] (2,2) -- (3,2);
        \draw[->-,very thick,RoyalBlue] (3,2) -- node [black,right] {$\mathcal S_2$} (3,1);
        \draw[->-,very thick,RoyalBlue] (3,1) -- (2,1);

        %left plaq
        \draw[->-,thick,dashed,black] (1.2,1.2) -- (1.8,1.2);
        \draw[->-,thick,dashed,black] (1.8,1.2) -- (1.8,1.8);
        \draw[->-,thick,dashed,black] (1.8,1.8) -- (1.2,1.8);
        \draw[->-,thick,dashed,black] (1.2,1.8) -- (1.2,1.2);
        \node at (1.5,1.5) {$\mathcal P_1$};

        %right plaq
        \draw[->-,thick,dashed,black] (2.2,1.2) -- (2.8,1.2);
        \draw[->-,thick,dashed,black] (2.8,1.2) -- (2.8,1.8);
        \draw[->-,thick,dashed,black] (2.8,1.8) -- (2.2,1.8);
        \draw[->-,thick,dashed,black] (2.2,1.8) -- (2.2,1.2);
        \node at (2.5,1.5) {$\mathcal P_2$};

        %\node[anchor={north east}] at (1,1) {$\mathcal P_1$};
        %\node[anchor={north east}] at (2,1) {$\mathcal P_2$};

    \end{tikzpicture}
    \caption{Staples connected to a vertical link}
    \label{fig:staples}
\end{figure}

The contribution of $\mathcal P_1, \mathcal P_2$ to the action can be written in terms of
the link variable $u \equiv U[\mathcal L]$ in the following way:
\[\begin{aligned}
    \Re\,U[\mathcal P_1] + \Re\,U[\mathcal P_2] &= \Re\,(uU[\mathcal S_1]) + \Re\,(u^*U^*[\mathcal S_2]) \\
                                                &= \Re\,(Wu)
\end{aligned}\]
where $W \equiv U[\mathcal P_1] + U[\mathcal P_2] \notin U(1)$ has a modulus $|W|\neq1$.
Thus, the marginal distribution of $u$ with all the other link variables kept fixed at $u_\mathrm{fix}$ is:
\[\begin{aligned}
    p(u;u_\mathrm{fix}) &\propto e^{-\beta(1-\Re(Wu))} \\
                        &\propto e^{\beta\Re(Wu)} \\
                        &= e^{\beta|W|\Re\left(u_0\right)}
\end{aligned}\]
Here, $u_0 \equiv \frac{W}{|W|}u \in U(1)$, and $\mathrm du_0=\mathrm du$ are then the same Haar measure of the compact group $U(1)$.
Which means:
\[
    p(u;u_\mathrm{fix}) = p(u_0;u_\mathrm{fix}) \propto e^{\beta|W|\Re\left(u_0\right)}
\]
Indeed, $u$ can be parameterized as $u\equiv e^{i\phi}$ in which $\phi$ is defined in $(-\pi + \phi_0, \pi + \phi_0]$ with a generic $\phi_0$.
After a transformation $u \rightarrow \frac{W}{|W|}u$, $\phi$ undergoes the translation $\phi \rightarrow \phi + \arg W$.
If the parameterization is redefined as $\phi_0 \rightarrow \phi_0 - \arg W$, the shift of the definition interval of $\phi$ is then reabsorbed.

Choosing to parameterize $u_0 \equiv e^{ix}$ with $x \in (-\pi, \pi]$,
the marginal distribution of $x$ finally is:
\begin{equation}\label{eq:local_pdf}
    p(x;u_\mathrm{fix}) \propto e^{k\cos(x)} \quad x \in (-\pi, \pi]
\end{equation}
with $k \equiv \beta|W|$.
Equation \eqref{eq:local_pdf} cannot be integrated analytically since its integral is \cite{nist:2010}:
\[
    \int_{-\pi}^{\pi}\mathrm dx\,e^{k\cos(x)} = 2\pi I_0(k)
\]
where $I_0(k)$ is the modified cylindrical Bessel function with index $0$.
Therefore, it is not possible to invert it exactly.

There are methods that make usage of a numerical approximations of the inverse function \cite{bazavov:2005},
but these solutions offer no significant advantages compared with the method that has been adopted,
which is simpler, and even more efficient if $\beta$ is high valued.

It was chosen to sample it using a Metropolis-Hastings algorithm,
and, for what has been said at the end of Chapter \ref{ch:mc},
the acceptance of the algorithm increases as much similar $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ gets to $p(x_\mathrm{new};u_\mathrm{fix})$.

On this matter, Laplaces's method identifies such a distribution.
Indeed it is its corollary that any unimodal distribution of the form $\propto e^{kf(x)}$ approaches a Gaussian as $k$ increases.

In particular, it proves that (Figure \ref{fig:gauss}):
\begin{equation}\label{eq:gauss_approx}
    e^{k\cos(x)} \stackrel{k\to\infty}{\scalebox{2}[1.25]{$\sim$}} e^{-kx^2/2}
\end{equation}

\begin{figure}[!htb]
    \centering
    \input{gfx/gauss.pgf}
    \caption{$p(x;u_\mathrm{fix})$ converges to a Gaussian as $k$ increases}
    \label{fig:gauss}
\end{figure}

Laplace's method states that, if $f(x)$ is a twice-differentiable function that has only one maximum point $x_0 \in [a,b]$ with $x_0 \neq a,b$,
then:
\begin{equation}\label{eq:laplace}
	\int_a^b\mathrm dx\,e^{kf(x)} \xrightarrow{k\to\infty} e^{kf(x_0)}\int_a^b\mathrm dx\,e^{kf"(x_0)(x-x_0)^2/2}
\end{equation}
Indeed, as $k$ increases, the points that are further from the maximum point $x_0$ are exponentially suppressed,
and only the points in the range of $x_0$ becomes relevant to the integral.
The order zero of the Taylor expansion of $f(x)$ around $x_0$ is a multiplicative constant and the order one does not contribute since it is symmetric.
The meaningful part is then the second order, which is negative since $x_0$ is a maximum,
and hence the integrand of the second term of Equation \eqref{eq:laplace} is a Gaussian.

Applying Laplace's method for the case of Equatiton \eqref{eq:local_pdf},
the global maximum is $x_0=0$, and $f"(x_0) = -1$.
Thus:
\[
    \int_{-\pi}^x\mathrm dx'\,e^{k\cos(x')} \stackrel{k\to\infty}{\scalebox{2}[1.25]{$\sim$}} \int_{-\pi}^x\mathrm dx'\,e^{-k{x'}^2/2} \quad \mathrm{for}\ x>0
\]
and equation \eqref{eq:gauss_approx} is recovered taking the derivative of both members, and extending the results for symmetry around $0$.

\subsection*{Gaussian sampling}
The proposal distribution has then been chosen to be:
\begin{equation}\label{eq:gauss_proposal}
    \mathcal P(y\to x) \propto e^{-kx^2/2} \quad x \in (-\pi,\pi]
\end{equation}

A Gaussian in $(-\infty,\infty)$ can be directly sampled using the Box-Muller method,
and an adaptation of it is needed to restrict the variable into $(-\pi,\pi)$.

The probability density function of two independent random variables $x_1,x_2$ distributed according to $\mathcal P(y\to x)$ is:
\begin{equation}\label{eq:gauss2d}
    p(x_1,x_2) \propto e^{-\frac{k}{2}(x_1^2+x_2^2)}
\end{equation}

Switching to polar coordinates\footnote{The correct extreme values of the interval of definition will be recovered afterwards, adapting the final solution.}:
\[\begin{aligned}
    &\begin{dcases}
        x_1 = r \cos\theta \\
        x_2 = r \sin\theta
    \end{dcases}
    &
    \begin{dcases}
        r \in (0,\pi] \\
        \theta \in [0,2\pi)
    \end{dcases}&
\end{aligned}\]
the distribution in terms of $r$ and $\theta$ becomes:
\[
    p(r,\theta) \propto re^{-\frac{k}{2}r^2}
\]
which is separable into two independent distributions:
\[\begin{aligned}
    p(r,\theta) &= p(r)p(\theta) \\
                &= \left(\mathcal Nre^{-\frac{k}{2}r^2}\right)\left(\frac{1}{2\pi}\right)
\end{aligned}\]
where $\mathcal N$ is a normalization constant.

$p(\theta)$ is a uniform distribution and is then trivial to sample.
To directly sample $p(r)$, instead, can be used the procedure defined in Chapter \ref{ch:mc}.
Indeed, integrating it:
\[
    F(r) = \int_0^r\mathrm dr'\,p(r') = \frac{\mathcal N}{k}\left.e^{-\frac{k}{2}{r'}^2}\right|_r^0 = \frac{\mathcal N}{k}\left(1-e^{-\frac{k}{2}r^2}\right)
\]
and evaluating its inverse $F^{-1}(y_1)$:
\[\begin{aligned}
    y_1 &= F(r) = \frac{\mathcal N}{k}\left(1-e^{-\frac{k}{2}r^2}\right) \\
        &\Rightarrow e^{-\frac{k}{2}r^2} = 1 - y_1\frac{k}{\mathcal N} \\
        &\Rightarrow F^{-1}(y_1) = r = \sqrt{-\frac{2}{k}\log\left(1-\frac{k}{\mathcal N}y_1\right)}
\end{aligned}\]

It remains to evaluate the normalization constant:
\[
    \mathcal N^{-1} = \int_0^\pi\mathrm dr\,re^{-\frac{k}{2}r^2} = \frac{1}{k}\left(1-e^{-\frac{k}{2}\pi^2}\right)
\]
to get:
\[
    F^{-1}(y_1) = \sqrt{-\frac{2}{k}\log\left[1-y_1\left(1-e^{-\frac{k}{2}\pi^2}\right)\right]}
\]
which means that if $y_1, y_2$ are equally distributed in $(0,1)$, then:
\begin{equation*}
    \begin{dcases}
        x_1 = F^{-1}(y_1)\cos(2\pi y_2) \\
        x_2 = F^{-1}(y_1)\sin(2\pi y_2)
    \end{dcases}
\end{equation*}
are two independent variables distributed according to \eqref{eq:gauss2d}.

However they do not span homogeneously all the interval $(-\pi,\pi]$ if the probability of being exactly at points $0$ and $\pi$ is taken into account,
which is non-zero in numerical applications because the representation of numbers in computer memory is finite.

To obtain a random variable $x = r\cos \theta \in (-\pi,\pi]$,
it is necessary that $r \in [0,\pi]$ and $\theta \in (-\pi,\pi)$.

Since $F^{-1}(0)=0$ and $F^{-1}(1)=\pi$,
a completely uniform variable $x$ distributed according to Equation \eqref{eq:gauss_proposal} is given by:
\begin{equation}\label{eq:gauss_angle}\begin{gathered}
    x = \sqrt{-\frac{2}{k}\log\left[1-y_1\left(1-e^{-\frac{k}{2}\pi^2}\right)\right]}\cos\left[2\pi\left(y_2-\frac{1}{2}\right)\right] \\
    \mathrm{with}\ \begin{dcases}
                         y_1 \in [0,1] \\
                         y_2 \in (0,1)
                   \end{dcases}
\end{gathered}\end{equation}

\subsection*{Local Metropolis-Hastings}
Now that the proposal distribution $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ has been chosen,
the local algorithm can be defined as follows:
\begin{itemize}
    \item Select one link $\mathcal L$.
    \item Identify $\mathcal S_1$ and $\mathcal S_2$, the staples connected to $\mathcal L$ (Figure \ref{fig:staples}).
    \item Compute $W$, and then $k$ and $x_\mathrm{old}$:
        \[\begin{aligned}
            &W = U[\mathcal S_1] + U[\mathcal S_2],
            &
            &\begin{dcases}
                k = \beta |W| \\
                x_\mathrm{old} = \arg\left(WU[\mathcal L]\right)
            \end{dcases}
        \end{aligned}\]
    \item Extract $x_\mathrm{new}$ from $\mathcal P(x_\mathrm{old}\to x_\mathrm{new})$ using the algorithm of Equation \eqref{eq:gauss_angle}.
    \item Accept the proposal with probability (Equations (\ref{eq:acceptance}, \ref{eq:local_pdf}, \ref{eq:gauss_proposal})):
        \[\begin{aligned}
            \mathcal A(x_\mathrm{old}\to x_\mathrm{new})
                 &=  \min\left\{\frac{p(x_\mathrm{new};u_\mathrm{fix})\mathcal P(x_\mathrm{new}\to x_\mathrm{old})}
                                     {p(x_\mathrm{old};u_\mathrm{fix})\mathcal P(x_\mathrm{old}\to x_\mathrm{new})},1\right\} \\[.5em]
                 &= \min\left\{e^{k\left(\cos(x_\mathrm{new})-\frac{1}{2}x_\mathrm{old}^2-\cos(x_\mathrm{old})+\frac{1}{2}x_\mathrm{new}^2\right)},1\right\}
        \end{aligned}\]
        That is, generate a random variable $p$ from the uniform distribution in $[0,1)$,
        and accept the proposal if:
        \[
            p < e^{k\left(\cos(x_\mathrm{new})-\frac{1}{2}x_\mathrm{old}^2-\cos(x_\mathrm{old})+\frac{1}{2}x_\mathrm{new}^2\right)}
        \]
    \item If the proposal is accepted, set the corresponding new value for the link variable $U[\mathcal L]$
        \[
            U[\mathcal L] \mapsfrom e^{i(x_\mathrm{new}-\arg W)}
        \]
\end{itemize}

The local algorithm should then iterate over all links in the lattice to be ergodic, as discussed in Chapter \ref{ch:mc}.
Every sweep over all lattice links will be considered a step of the Markov chain,
and the expectation values will be evaluated on this sequence.

\subsection*{Local algorithm implementation}
Here is provided a C++17 implementation of the local algorithm using the {\ttfamily Lattice} class introduced in Chapter \ref{ch:lattice}.

It is necessary to define first a way to sample the proposal distribution of Equation \eqref{eq:gauss_proposal} and the uniform distribution.

In the C++ Standard Library, probability distributions are defined in terms of a generic templated type {\ttfamily URNG},
and the same convention will be used here.

The uniform distribution is already present in the Standard, implemented as the {\ttfamily uniform\_real\_distribution} class,
and, with its default parameters, corresponds to the uniform distribution in the interval $[0,1)$.
It will be aliased as follows:
\begin{lstlisting}[caption={Uniform distribution sampling function}]
using UniformDouble = uniform_real_distribution<double>;
\end{lstlisting}

The algorithm of Equation \eqref{eq:gauss_angle} can be implemented using the interface of Standard's URNGs:
\begin{lstlisting}[caption={Gaussian angle distribution sampling function}]
template <class URNG>
double gauss_angle(double k, URNG &rng)
{   
    // y1 unif in [0,1], y2 unif in (0,1)
    double y1 = (double)(rng()-rng.min())/(rng.max()-rng.min());
    double y2 = (double)(rng()-rng.min()+1lu)
                         / (rng.max()-rng.min()+2lu);
    
    double r = sqrt(-2./k*log(1.-y1*(1.-exp(-0.5*k*pi*pi))));
    double theta = 2.*pi*(y2-0.5);
    
    return r*cos(theta);
}
\end{lstlisting}

The local algorithm makes use of staples objects.
The same paradigm used in Chapter \ref{ch:lattice} is employed here to define the {\ttfamily Staple} type.
A staple can be identified given its connected link and the direction of the staple's first link.
For example, the staples $\mathcal S_1$ and $\mathcal S_2$ of Figure \ref{fig:staples} can be identified respectively with
$(\mathcal L, -1)$ and $(\mathcal L, 1)$.
The two connected staples employed in the local algorithm can then be obtained from the link $\mathcal L$ and its two ortogonal directions.

All this staples interface is implemented in the following way:
\begin{lstlisting}[caption={Staple type}]
using Staple = array<Link,3>;

// nu is the first staple link direction
Staple conn_staple(Link link, int nu)
{
    auto [s,mu] = link;
    return Staple{Link{s+hat(mu),nu},
                  Link{s+hat(mu)+hat(nu),-mu},
                  Link{s+hat(nu),-nu}};
}

// The order of returned staples is not considered
array<Staple,2> conn_staples(Link link)
{
    int nu;
    // Select ortogonal directions
    switch (link.mu) {
        case 1:; case -1: nu = 2; break;
        case 2:; case -2: nu = 1; break;
        default: throw runtime_error("Invalid mu");
    }
    return array<Staple,2>{conn_staple(link,nu),
                           conn_staple(link,-nu)};
}
\end{lstlisting}

Before updating links, it is recommended to start from a \emph{hot} configuration,
\ie with all link variables sampled uniformly in $U(1)$.
This starting configuration usually leads to shorter thermalization times.
\begin{lstlisting}[caption={Hot configuration}]
template <class URNG>
void set_hot(Lattice &lat, URNG &rng)
{
    for (int mu : {1,2}) {
        for (Site s : lat.sites()) {
            double theta = UniformDouble(0.,2.*pi)(rng);
            lat.set_link(Link{s,mu},exp(1i*theta));
        }
    }
}
\end{lstlisting}

The local algorithm is finally:
\begin{lstlisting}[caption={Local algorithm}]
// Return 1.0 if move is accepted, 0.0 if not
template <class URNG>
double local_update(Lattice &lat, Link link, URNG &rng)
{
    auto [S_1,S_2] = conn_staples(link);
    
    cmplx W = lat.s_line(S_1) + lat.s_line(S_2);
    double k = lat.beta()*abs(W);
    double x_old = arg(W*lat.s_line(link));
    
    double x_new = gauss_angle(k,rng);
    
    double p = exp(k*(cos(x_new)+pow(x_new,2)/2.
                     -cos(x_old)-pow(x_old,2)/2.));
    
    if (UniformDouble()(rng)<p) {
        cmplx u_new = exp(1i*(x_new-arg(W)));
        lat.set_link(link,u_new);
        return 1.;
    }
    else return 0.;
}
\end{lstlisting}

The local algorithm is then iterated over each lattice link:
\begin{lstlisting}[caption={Local sweep}]
template <class URNG>
double local_sweep(Lattice &lat, URNG &rng)
{
    double accept = 0.;
    for (int mu : {1,2}) {
        for (Site s : lat.sites()) {
            accept += local_update(lat,Link{s,mu},rng);
        }
    }
    return accept/pow(lat.N(),2)/2.;
}
\end{lstlisting}
A local sweep will be considered as an iteration of the Markov chain.

\section{First results}

The simulations have been run with parameters $\beta$ and $N$ on the fixed physical volume line $\beta/N^2=1/80$,
which is the same line considered by D\"urr and Hoelbling \cite{durr-hoelbling:2005}.
The results will than be compared with the ones reported in their article.

%\begin{table}[!htb]
%    \centering
%    \input{tables/local_cont.tex}
%    \caption{Parameters considered for the local algorithm}
%    \label{tab:local_cont}
%\end{table}

\subsection*{Metropolis-Hastings acceptance}

As expected, the Metropolis-Hastings acceptance converges to 1 when approaching the continuum limit (Figure \ref{fig:local_acc}),
since, if $\beta$ increases, the $k\propto\beta$ parameter of Figure \ref{fig:gauss} increases, and the Laplace's method becomes more accurate.

\begin{figure}[!htb]
    \centering
    \import{gfx/}{local_acc.pgf}
    \caption{Local algorithm acceptance}
    \label{fig:local_acc}
\end{figure}

\subsection*{Plaquette mean energy}
For a $24\times24$ lattice with $\beta=7.2$, it is reported in \cite{durr-hoelbling:2005} the following value for the plaquette mean energy:
\begin{equation}\label{eq:plaq_durr}
    E[\mathcal P] = 0.52040(39) \quad \text{(D\"urr-Hoelbling)}
\end{equation}

Running the local algorithm for $10^6$ iterations, and discarding the first $20\%$ of the data as thermalization,
the following value of the plaquette energy has been obtained:
\begin{equation}\label{eq:plaq_local}
    E[\mathcal P] = \input{tables/plaq_local.tex} \quad \mathrm{(Local\ algorithm)}
\end{equation}
which is compatible with \eqref{eq:plaq_durr}.

However, problems arise in the attempt to evaluate the continuum limit of the plaquette mean energy (Figure \ref{fig:local_energy_cont}).
The values of the two points on the left seem to be erroneous even if the plot of the plaquette energies history
does not show any evident problem (Figure \ref{fig:local_energy_history}).

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \import{gfx/}{local_energy_cont.pgf}
    \end{subfigure}\vspace{1.5em}
    \begin{subfigure}{\textwidth}
        \centering
        \input{tables/local_energy_cont.tex}
    \end{subfigure}
    \caption{Biased plaquette energy continuum limit obtained with the local algorithm}
    \label{fig:local_energy_cont}
\end{figure}

\begin{figure}[!htb]
    \centering
    \import{gfx/}{local_energy_history.pgf}
    \caption{Plaquette energy history obtained with the local algorithm}
    \label{fig:local_energy_history}
\end{figure}

The reason of these biased measures can be seen examining the behaviour of the topological charge.

\subsection*{Topological freezing}

Plotting the histories of the total topological charge values, the problem can be identified.
The second last plot of Figure \ref{fig:local_charge_history} shows that the topological charge remains at the same value for a very long number of iterations,
while the last plot shows a complete charge freezing.

\begin{figure}[!htb]
    \centering
    \import{gfx/}{local_charge_history.pgf}
    \caption{Topological charge history obtained with the local algorithm}
    \label{fig:local_charge_history}
\end{figure}

The critical behaviour of the integrated autocorrelation time can be seen in Figure \ref{fig:local_charge_corr}.
The last two points are excluded from this plot because of the insufficient number of independent configurations.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \import{gfx/}{local_charge_corr.pgf}
    \end{subfigure}\vspace{1.5em}
    \begin{subfigure}{\textwidth}
        \centering
        \input{tables/local_charge_corr.tex}
    \end{subfigure}
    \caption{Divergent integrated autocorrelation time of the topological charge obtained with the local algorithm}
    \label{fig:local_charge_corr}
\end{figure}

The topological susceptibility $\chi$ is proportional to the second moment of the topological charge:
\[
    \frac{\chi}{g^2} = \frac{\left<Q^2\right>}{a^2g^2N^2} = \frac{\left<Q^2\right>}{N^2}\beta
\]
Thus, the extrapolation of the continuum limit is very problematic in the presence of topological freezing (Figure \ref{fig:local_susc_cont}).

\begin{figure}[!htb]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \import{gfx/}{local_susc_cont.pgf}
    \end{subfigure}\vspace{1.5em}
    \begin{subfigure}{\textwidth}
        \centering
        \input{tables/local_susc_cont.tex}
    \end{subfigure}
    \caption{Biased topological susceptibility continuum limit obtained with the local algorithm}
    \label{fig:local_susc_cont}
\end{figure}

All these problems will be completely overcome with the introduction of the cluster algorithm described in the next Chapter.


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
