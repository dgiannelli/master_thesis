%*******************************************************
% Preface
%*******************************************************
\phantomsection
\markboth{\spacedlowsmallcaps{Preface}}{\spacedlowsmallcaps{Preface}}
\pdfbookmark[1]{Preface}{Preface}
\addcontentsline{toc}{chapter}{\tocEntry{Preface}}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Preface}

\section*{The renascence of statistical sampling}
Nicholas Metropolis defined in this way \cite{metropolis:1987} the period that began in 1945, after the building of ENIAC, the first general purpose electronic computer.
Its first usage was in relation to the problem of the nuclear chain reaction.
The success obtained brought a renascence of a mathematical technique known as statistical sampling, and its usage spread with a new name: the Monte Carlo method.
This name was used in scientific literature for the first time in 1949 by Metropolis and Ulam \cite{metropolis-ulam:1949}.

The main idea of the Monte Carlo method is based on pseudorandom numbers, which are sequences of integer numbers whose properties approximate the properties of random numbers sequences.
From pseudorandom numbers, it is possible to sample sequences of real numbers that are distributed according to a given probability density function.
The applications of this principle are very broad: from numerical approximations of integrals and differential equations, to simulation of events that have a probabilistic nature.

The impact on Physics was enormous. Theoretical physicists not only have a powerful method to compute the predictions of a theory, but is also possible to choose, from a wide-ranging class of theories, the one that is most compatible with experiments.
The Monte Carlo method is also an essential tool for experimental physicists, who can simulate an experimental setup in order to estimate its performance, or to compare the results with the actual experimental results to find if there are problems with the apparatus, or maybe, to discover weakness in physics theories and start working to expand them.

\section*{From local algorithms to cluster algorithms}
A class of problems in which the Monte Carlo method shines is the integral in a very high number of dimensions.
Other methods, like the quadrature rules, suffer the so called \emph{curse of dimensionality}: the error of the approximation in terms of the number of function evaluations scales exponentially with the number of dimensions.
For Monte Carlo, on the other hand, the scaling is independent from the number of dimensions.

A branch of Physics in which such integrals are very frequent is Statistical Mechanics.
In order to compute mean values of observables at thermodynamic equilibrium,
it is necessary to integrate the observable in all configuration in the phase space, weighted in accordance with the Boltzmann distribution.
If it is possible to sample the configurations from the Boltzmann distribution,
the arithmetic mean of the observable evaluated in all these configurations will converge to the correct result of the integral.

A very important step forward in this topic was achieved when Metropolis et al. introduced in 1953 \cite{metropolis:1953} the Metropolis algorithm,
which was further generalized by Hastings in 1970 \cite{hastings:1970} and it is usually referred as Metropolis-Hastings algorithm.
Applying an accept/reject principle, it allows to generate Markov chains of configurations in the phase space that follow a given probability density function.
Its generality and simplicity potentially allow to compute almost any integral that appears in Statistical Mechanics with an improved convergence rate.

An important challenge in Markov chain sampling is achieving ergodicity,
\ie the property according to which the sequence will eventually reach every point in the phase space in a finite number of steps.
Without ergodicity, the Markov chain is actually sampling only a subset of the domain, thus, the integral result will probably be incorrect.
Even if the algorithm is ergodic, if the number of steps needed to reach all relevant sectors of the phase space is too high
with respect to the number of iterations the computer can perform in the available amount of time, the algorithm will effectively behave as it is not ergodic.

An example of this problem is the two dimensional Ising model near the critical temperature.
This system comprises a lattice of spins which can be oriented in two directions.
Each pair of adjacent spins gives a contribute to the total energy: if they are aligned, the contribute is negative, otherwise it is positive.
At thermodynamic equilibrium, if the temperature is above the critical temperature and in absence of external magnetic fields, the mean magnetization of the system is zero.
Below the critical temperature, most of the spins are aligned in the same direction, producing a non-zero mean magnetization.
Varying the temperature in a range of values that are still greater than the critical temperature, but very close to it,
it is possible to observe a critical behaviour: distinct cluster of spins aligned internally all together start arising.
The size of these clusters keeps increasing as the temperature approaches the critical value.

In terms of a Monte Carlo simulation, this cluster structure has important implications.
The energy of the system has a local nature,
\ie it is expressed in terms of the single spins, and, modifying the value of a spin,
the variation of the total energy is only due to the local energy variation.
Because of this fact, the most straightforward way to sample all the configurations is to update all the spins one by one,
changing only one spin at each iteration of the program. This is what is called a local Monte Carlo.
However, when the system is divided into clusters, local algorithms show their weakness.
Updating the spins one per time makes very unlikely that a whole cluster can be reversed,
and this problem gets worse when the cluster size increases.
This is the \emph{critical slowing down}: the correlation length between the configurations increases exponentialy approaching the critical temperature.

A solution to the slowing down for the Ising model\footnote{It applies also to the more general Potts model} was found by Swendsen and Wang in 1987 \cite{swendsen-wang:1987}.
They found a way to sample the Boltzman distribution inverting many contiguous and parallel spins all together.
It is recorded as the first cluster algorithm.
In 1989, Wolff \cite{wolff:1989} improved this algorithm and extended it also to continuous spin models.

Cluster algorithms have a chance to update big groups of elements, and, if they are well defined,
the size of these groups is scalable together with the typical size of the clusters present in the system.
\emph{Critical slowing down} can then be heavily dampened, making the simulations affordable even in critical conditions.

However, the difficulty in creating cluster algorithms is to find a way to efficiently update big parts of the system without compromising the Boltzmann distribution.
This has to be done exploiting properties of the considered system, and, consequently, the algorithm is specifically tailored for systems that share those properties.
For example, Wolff algorithm can be extended to the most generic continuous spin system on a lattice,
but any good solution was not found to extend it to the hard sphere model,
\ie a system in which solid spheres can move in space without overlapping.
The first efficient cluster algorithm for this model was found 1995 by Dress and Krauth \cite{dress-krauth:1995}.
It is called \emph{the pivot algorithm} and was formulated specifically for hard spheres and related systems\footnote{Like binary mixtures or dimers, for example},
independently from the efforts that have been made to extend Wolff algorithm.

Another important case of \emph{critical slowing down} is \emph{topological freezing}.
It affects Quantum Cromodynamics\footnote{As well as other quantum theories with a non-trivial topological structure}
when is studied with the Monte Carlo method in its lattice formulation, \ie Lattice QCD.

\section*{Lattice QCD and topological freezing}
Quantum Cromodynamics is the quantum field theory that describes the \emph{strong interation},
which concerns the coupling between \emph{quarks} (the constituents of nucleons and mesons) and \emph{gluons} (massless particles, mediators of the interaction).
It is formulated as a non-abelian gauge theory, with gauge group SU(3),
in which quark fields lie in the foundamental representation of the group and gluon fields are the gauge fields.
The conserved charge associated with the gauge group is called \emph{color charge} and is the analogous of the \emph{electric charge} in electrodynamics.

This formulation for the theory of strong interation was introduced in 1973 by Fritzsch, Leutwyler and Gell-Mann \cite{fritzsch:1973},
and it is based on the general field theory developed by Yang and Mills in 1954 \cite{yang-mills:1954}.
It came after years of important theoretical contributions that followed the experimental discovery of a growing number of hadrons in 1950s.

In 1973, Gross, Wilczek \cite{gross-wilczek:1973} and Politzer \cite{politzer:1973} discovered an important property of QCD: \emph{asymptotic freedom}.
As the energy scale increases, the strenght of the interaction decreases,
and physical predictions of the theory can be analytically computed with a perturbative power expansion of the coupling constant.

On the other side of the spectrum, when the energy scale decreases, \emph{color confinement} arises.
Quarks and gluons cannot be isolated and observed directly in normal conditions\footnote{It is only possible at extremely high temperature and/or density, when they form a quark-gluon plasma, a new state of matter},
and they are confined in mesons or nucleons.
As coupling is higher at this scale of energy, perturbation theory is not viable, and non-perturbative methods are needed to compute theory predictions.
Lattice QCD is a different approach that fulfils these requirements, and it was formulated in 1974 by Wilson \cite{wilson:1974}.
Gauge field theories can be quantized on a discrete lattice in euclidean space-time and,
in this framework, it is possible to compute theory predictions in the strong coupling regime.

In 1979, Creutz et al. \cite{creutz:1979} and Wilson \cite{wilson:1980}
had the intuition that gauge theories on a finite euclidean lattice are equivalent to a system at thermodynamic equilibrium,
and mean values of observables can be computed with the same Monte Carlo techniques that had already been employed in Statistical Mechanics.
The continuum limit can then be extrapolated with the \emph{renormalization group} method, performing simulations on finer and finer lattices.
Following these precursor works, lattice QCD simulations produced in short time important results in evaluating quark-antiquark potential and hadron masses
\cite{creutz:1980, hamber-parisi:1981, weingarten:1982}.

However, there is a problem in extrapolating the continuum limit.
QCD has a non-trivial topological structure, and gluon field configurations are partitioned in homotopy classes, characterized by their integer topological charge.
It is then impossible to continuously deform a configuration with a charge value into another with a different charge.
When the theory is discretized on a lattice, it becomes possible to tunnel from one class to another,
though the effort is harder as continuum is approached.

In the context of Markov chain Monte Carlo simulations, homotopy class partitioning results in a loss of ergodicity.
The correlation time of topological charge increases exponentially as the lattice gets finer.
This is an instance of \emph{critical slowing down}, and is called \emph{topological freezing}.

The demand for an efficient topological charge sampling is very high, and it is a problem that is currently not resolved.
It is of a great physical interest, especially because there are particle masses directly related to topological charge:
the $\eta'$ meson mass, whose high value has an explaination with the Witten-Veneziano mechanism \cite{witten:1979-1, witten:1979-2, veneziano:1979},
and the mass of the axion, which is an hypothetical particle postulated by Weinberg \cite{weinberg:1978} and Wilczek \cite{wilczek:1978}
in the context of Peccei-Quinn model \cite{peccei-quinn:1977-1, peccei-quinn:1977-2}.
The weakness of the coupling between the axion and the Standard Model, makes it a good candidate to be \emph{dark matter}.
The experimental dicovery of the axion could be a great step in understanding the nature of \emph{dark matter}, which is one of the main problems in modern Physics.
An accurate value of the axion mass from lattice QCD simulations would indicate the point of the energy spectrum in which the axion should be,
and its existence could be verified experimentally.

The topological charge is the integral of a local quantity, the topological charge density%
\footnote{In QCD, topological charge density rose from studies about anomalous symmetries, as it is proportional to the anomaly of the axial U(1) symmetry},
which is a functional of gauge fields configurations.
Studying its spatial distribution in lattice QCD simulations close enough to continuum limit, in condition of \emph{topological freezing}, it exhibits an interesting structure.
Some regions of the space contribute with a certain topological charge, which can be positive or negative, and,
fast forwarding the configurations of the Markov chain,
those regions may deform and move around in the space, but their single contribute to the total charge doesn't change.
They are the discretized version of instantons, which are exact solution of the classical equation of motion.
They have a definite integer topological charge, and their existence proves the non-triviality of QCD topology.

The cluster stucture of topologically frozen configurations suggests that a cluster algorithm would be the perfect solution to the problem.
However, it hasn't been found yet, and only partial solutions of the problem have been developed.%
\marginpar{Chiedere ai prof. una breve descrizione dei principali metodi usati per aggirare il top. freezing}

\section*{A toy model for QCD}
Lattice QCD simulations are, in general, difficult to implement and computationally very expensive.
For this reason, many new ideas and algorithms are firstly applied to simplified models that share the same properties.
This can be useful both for testing and benchmarking purposes.

In particular, in order to study the problem of \emph{topological freezing},
it is needed a gauge field theory that has a good definition of local topological charge and exhibits topological freezing in the form of the cluster stucture described before.
The simplest model that share these properties with QCD is the two dimensional pure gauge U(1) theory, which is the subject of this thesis.

A new cluster algorithm for this model will be presented.
It constructs closed paths lattice links and reverse the sign of all gauge fields configurations in the region surrounded by the path,
accepting or rejecting the operation with the Metropolis-Hastings algorithm.
A well targeted use of local gauge invariance can make the acceptance for the cluster inverting to be the same that would be due to a local update,
which is reasonably high%
\marginpar{Aggiungere l'andamento dell'accettanza con $\beta$}
and independent of the cluster size.

This algorithm is not extensible to more than two dimensions,
because regions of space could not be completely surrounded with a one dimensional path.
However, it solves the critical behaviour in two dimensions, and it is efficient, parallelizable, and simple to implement, especially with modern programming languages techniques.
It yields exact results that can be used to test all the workarounds that are used to fight the \emph{topological freezing} in QCD. 

The only other cluster algorithm for this theory that is present in literature was published in 1992 by Robert Sinclair \cite{sinclair:1992}.
His algorithm uses the local gauge invariance to map all the system to a one dimensional XY model, and updates it using the extension of the Wolff algorithm.
Although it is a valid solution, the work of Dress and Krauth in 1995 proves that, in some cases, instead of extending Wolff algorithm,
it may be simpler and more efficient to create a new algorithm that is well suited for the specific task, and, being closer to the underlying Physics,
it may be easier to debug and understand.
\ \\ \ \\
The exposure of the thesis work will fit in the following scheme:
\begin{description}
    \item{Chapter \ref{ch:toy_model}:} Definition of the two dimensional pure gauge U(1) theory and its topological charge density,
    both in the continuum space and in their discretized version on a lattice.
    \item{Chapter \ref{ch:lattice}:} Implementation of the lattice theory that has been used.
    \item{Chapter \ref{ch:local}:} Formulation and implementation of a local Metropolis-Hastings algorithm.
    \item{Chapter \ref{ch:cluster}:} Discussion of topological freezing, and introduction of a new cluster Metropolis-Hastings algorithm that solves the problem.
    \item{Chapter \ref{ch:results}:} Numerical results obtained with the cluster algorithm and conclusions.\\
    \item{Appendix \ref{ap:code}:} Complete implementation of the algorithms.
    \item{Appendix \ref{ap:data}:} Methods used for data analysis.
\end{description}

\endgroup

\vfill
